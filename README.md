# ğŸ§  Rag Chat Assistant

![Run Tests](https://github.com/alexanderiwoh/rag-chat-assistant/actions/workflows/test.yaml/badge.svg)

A simple, containerized Retrieval-Augmented Generation (RAG) system with a web-based chat interface.  
Built with **FastAPI**, **LangChain**, and **ChromaDB** to parse, embed, index, and retrieve content from PDF documents.

---

## ğŸš€ Features

- ğŸ§  LLM-powered agent using LangChain
- ğŸ“„ PDF ingestion, chunking, and embedding
- ğŸ” Semantic search with ChromaDB
- ğŸŒ Web-based UI to chat with your knowledge base
- ğŸ³ Fully containerized with Docker
- âš¡ï¸ One-liner startup via shell script

---

## ğŸ§± Tech Stack

- [FastAPI](https://fastapi.tiangolo.com/)
- [LangChain](https://www.langchain.com/)
- [ChromaDB](https://www.trychroma.com/)
- [Ollama](https://ollama.com/) â€“ local LLM server
- [Jinja2](https://jinja.palletsprojects.com/) + HTML/CSS for the chat UI
- [Docker + Compose](https://docs.docker.com/compose/)

---

## ğŸ’» Requirements

Before running the app, make sure you have the following installed:

- ğŸ³ [Docker](https://www.docker.com/products/docker-desktop)
- ğŸ¦™ [Ollama](https://ollama.com/download) â€“ for local LLM model inference

Both are cross-platform and free to install.

---

## ğŸš€ Starting the App

Once Docker and Ollama are installed and running, run the following in the Terminal:

```bash
./start-app
```

---

## ğŸ›‘ Stopping the App

Run the following in the Terminal:

```bash
./stop-app
```

---

## ğŸ§¹ Cleaning the App

Run the following in the Terminal:

```bash
./clean-app
```
